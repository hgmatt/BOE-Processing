# Databricks notebook source
# MAGIC %md
# MAGIC # BOE Spanish Document + Invoice Processing (Funhouse + SharePoint)
# MAGIC
# MAGIC This script processes Spanish BOE documents from SharePoint using native Funhouse objects
# MAGIC initialized by `%run /setup_python`.

# COMMAND ----------

# MAGIC %run /setup_python

# COMMAND ----------

import json
from datetime import datetime
from typing import Any, Dict, List

import pandas as pd
from funhouse.utils import extract_text

# COMMAND ----------

# Configuration (adjust as needed)
SHAREPOINT_FOLDER_PATH = "/Shared Documents/BOE"
ALLOWED_EXTENSIONS = (".pdf", ".jpg", ".jpeg", ".png", ".tif", ".tiff")
SKIP_EXTENSIONS = (".csv",)
MAX_TEXT_CHARS = 7000

FAM_BOE_GUIDELINES = """
Residential lease BOE allowability categories (U.S. Department of State FAM aligned):

ALLOWABLE (score 80-100):
- Building/common utilities tied to leased residence (water, gas, electric, sewage, trash)
- Mandatory building maintenance and common area fees
- Security, elevator, required building systems maintenance
- Mandatory HOA/condo/common charges required by lease

NEEDS REVIEW (score 50-79):
- Charges that might be lease-related but need lease-term confirmation
- Ambiguous repairs or mixed invoices with both allowable and personal items
- Fees that may be allowable only under specific post policy/lease language

NOT ALLOWABLE (score 0-49):
- Furniture, appliances, personal household services
- Personal internet/cable upgrades, entertainment, non-mandatory amenities
- Personal improvements, cosmetic upgrades, refundable deposits, penalties
- Vehicle or non-residential expenses
""".strip()

# COMMAND ----------

doc_client = fh_doc
translator = fh_translator
prompter = fh_prompter
sp_file = fh_sp_file

# COMMAND ----------


def build_classifier_system_prompt() -> str:
    return f"""You are a compliance analyst for BOE reimbursements on residential operating leases.
All source documents are in Spanish.

Task:
1) Extract invoice/expense details.
2) Output descriptions in ENGLISH.
3) Classify each expense using: allowable, needs review, not allowable.
4) Base classification on this policy:\n\n{FAM_BOE_GUIDELINES}

Return JSON only in this schema:
{{
  "vendor_name": "",
  "service_description": "",
  "service_type": "",
  "amount": "",
  "currency": "",
  "invoice_id": "",
  "invoice_date": "",
  "allowability_score": 0,
  "assessment": "allowable|needs review|not allowable",
  "assessment_reason": ""
}}
"""


def build_classifier_user_prompt(file_name: str, extracted_text: str, invoice_fields: Dict[str, Any]) -> str:
    return f"""Analyze this Spanish BOE document.

File name: {file_name}
Document text:
{extracted_text[:MAX_TEXT_CHARS]}

Document Intelligence invoice fields (if present):
{json.dumps(invoice_fields, ensure_ascii=False)}

Use the text and fields together. Keep output in English."""


def extract_invoice_fields(di_result: Dict[str, Any]) -> Dict[str, Any]:
    if not di_result or "documents" not in di_result or not di_result["documents"]:
        return {}

    fields = di_result["documents"][0].get("fields", {})
    normalized: Dict[str, Any] = {}

    for key in ["VendorName", "InvoiceId", "InvoiceDate", "InvoiceTotal"]:
        value = fields.get(key)
        if not value:
            continue
        if key == "InvoiceTotal" and value.get("valueCurrency"):
            normalized[key] = {
                "amount": value["valueCurrency"].get("amount"),
                "currency": value["valueCurrency"].get("currencyCode"),
            }
        else:
            normalized[key] = value.get("content") or value.get("valueString") or value.get("valueDate")

    return normalized


def run_document_intelligence(file_bytes: bytes) -> Dict[str, Any]:
    """Run multiple Document Intelligence models for broad coverage.

    Supports machine-readable PDFs, scanned files, and handwritten notes.
    """
    model_attempts = ["prebuilt-invoice", "prebuilt-read", "prebuilt-layout"]
    combined: Dict[str, Any] = {"content": "", "documents": []}

    for model_id in model_attempts:
        try:
            result = doc_client.analyze_document(document_bytes=file_bytes, model_id=model_id)
            if result.get("content") and len(result.get("content", "")) > len(combined.get("content", "")):
                combined["content"] = result["content"]

            if result.get("documents"):
                combined["documents"].extend(result["documents"])
        except Exception as model_error:
            print(f"  Document Intelligence model failed ({model_id}): {model_error}")

    return combined


def extract_best_available_text(file_path: str, file_bytes: bytes, di_result: Dict[str, Any]) -> str:
    """Extract text robustly across digital PDFs, scanned images, and handwriting."""
    di_text = (di_result or {}).get("content", "") or ""
    if di_text.strip():
        return di_text

    # AI-aware text extraction can route to OCR and handwriting-capable paths.
    ai_text = extract_text(file_bytes, ai=True)
    if ai_text and ai_text.strip():
        return ai_text

    # Vision OCR fallback for image-like docs and hard OCR edge cases.
    vision_text = fh_vision.analyze_text(file_bytes, return_plain_text=True)
    if vision_text and vision_text.strip():
        return vision_text

    # Final fallback: light text extraction from PDF bytes (machine readable content).
    text_fallback = extract_text(file_bytes, ai=False)
    if text_fallback and text_fallback.strip():
        return text_fallback

    # If still empty, provide a marker so downstream LLM has context.
    return f"[No text extracted from {file_path}. Potentially image-only or unsupported format.]"


def translate_if_needed(text: str) -> str:
    if not text:
        return ""
    return translator.translate(text=text, source_language="es", target_language="en") or text


def build_output_row(file_path: str, model_json: Dict[str, Any]) -> Dict[str, Any]:
    desc = model_json.get("service_description", "")
    reason = model_json.get("assessment_reason", "")

    return {
        "file_path": file_path,
        "vendor_name": model_json.get("vendor_name", ""),
        "description_of_service": translate_if_needed(desc),
        "type_of_service": model_json.get("service_type", ""),
        "amount": model_json.get("amount", ""),
        "currency": model_json.get("currency", ""),
        "invoice_id": model_json.get("invoice_id", ""),
        "date": model_json.get("invoice_date", ""),
        "allowability_assessment": model_json.get("assessment", "needs review"),
        "allowability_score": model_json.get("allowability_score", 0),
        "assessment_reason": translate_if_needed(reason),
        "processed_utc": datetime.utcnow().isoformat(),
    }


# COMMAND ----------

# Native SharePoint listing; no custom wrapper around list/download behavior
sp_items = sp_file.ls(SHAREPOINT_FOLDER_PATH, recursive=True, type="file", include_properties=False)
file_paths = [
    item if isinstance(item, str) else item.get("path") or item.get("serverRelativeUrl")
    for item in sp_items
]
file_paths = [p for p in file_paths if p and not p.lower().endswith(SKIP_EXTENSIONS)]

print(f"Found {len(file_paths)} non-CSV files in SharePoint folder: {SHAREPOINT_FOLDER_PATH}")

# COMMAND ----------

results: List[Dict[str, Any]] = []
system_prompt = build_classifier_system_prompt()

for idx, file_path in enumerate(file_paths, 1):
    print(f"Processing {idx}/{len(file_paths)}: {file_path}")
    try:
        if not file_path.lower().endswith(ALLOWED_EXTENSIONS):
            results.append(
                {
                    "file_path": file_path,
                    "vendor_name": "",
                    "description_of_service": "",
                    "type_of_service": "",
                    "amount": "",
                    "currency": "",
                    "invoice_id": "",
                    "date": "",
                    "allowability_assessment": "needs review",
                    "allowability_score": 0,
                    "assessment_reason": "Skipped file type not currently supported for extraction.",
                    "processed_utc": datetime.utcnow().isoformat(),
                }
            )
            continue

        content = sp_file.download_file(file_path, return_bytes=True)

        # Use multiple Document Intelligence models for machine PDFs, scans, and handwriting.
        di_result = run_document_intelligence(content)
        invoice_fields = extract_invoice_fields(di_result)
        extracted_text = extract_best_available_text(file_path, content, di_result)

        user_prompt = build_classifier_user_prompt(file_path.split("/")[-1], extracted_text, invoice_fields)
        response = prompter.chat(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.0,
            response_format="json",
            max_tokens=800,
        )
        model_json = json.loads(response)

        results.append(build_output_row(file_path, model_json))

    except Exception as ex:
        print(f"  ERROR: {ex}")
        results.append(
            {
                "file_path": file_path,
                "vendor_name": "",
                "description_of_service": "",
                "type_of_service": "",
                "amount": "",
                "currency": "",
                "invoice_id": "",
                "date": "",
                "allowability_assessment": "needs review",
                "allowability_score": 0,
                "assessment_reason": f"Processing error: {ex}",
                "processed_utc": datetime.utcnow().isoformat(),
            }
        )

# COMMAND ----------

results_df = pd.DataFrame(results)

display(results_df)

# Save output to SharePoint only (no local file writes)
output_sharepoint_path = f"{SHAREPOINT_FOLDER_PATH.rstrip('/')}/boe_invoice_assessments.csv"
sp_file.upload_bytes(results_df.to_csv(index=False).encode("utf-8"), output_sharepoint_path)
print(f"Uploaded output to SharePoint: {output_sharepoint_path}")
