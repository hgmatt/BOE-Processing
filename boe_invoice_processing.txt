# Databricks notebook source
# MAGIC %md
# MAGIC # BOE Spanish Document + Invoice Processing (Funhouse + SharePoint)
# MAGIC
# MAGIC This script processes Spanish BOE documents from SharePoint using native Funhouse objects
# MAGIC initialized by `%run /setup_python`.

# COMMAND ----------

# MAGIC %run /setup_python

# COMMAND ----------

import json
from datetime import datetime
from typing import Any, Dict, List

import pandas as pd
from funhouse.utils import extract_text

# COMMAND ----------

# Configuration (adjust as needed)
SHAREPOINT_FOLDER_PATH = "/Shared Documents/BOE"
ALLOWED_EXTENSIONS = (".pdf", ".jpg", ".jpeg", ".png", ".tif", ".tiff")
SKIP_EXTENSIONS = (".csv",)
MAX_TEXT_CHARS = 7000

FAM_BOE_GUIDELINES = """
Residential lease BOE allowability categories (U.S. Department of State FAM aligned):

ALLOWABLE (score 80-100):
- Building/common utilities tied to leased residence (water, gas, electric, sewage, trash)
- Mandatory building maintenance and common area fees
- Security, elevator, required building systems maintenance
- Mandatory HOA/condo/common charges required by lease

NEEDS REVIEW (score 50-79):
- Charges that might be lease-related but need lease-term confirmation
- Ambiguous repairs or mixed invoices with both allowable and personal items
- Fees that may be allowable only under specific post policy/lease language

NOT ALLOWABLE (score 0-49):
- Furniture, appliances, personal household services
- Personal internet/cable upgrades, entertainment, non-mandatory amenities
- Personal improvements, cosmetic upgrades, refundable deposits, penalties
- Vehicle or non-residential expenses
""".strip()

# COMMAND ----------

doc_client = fh_doc
translator = fh_translator
prompter = fh_prompter
sp_file = fh_sp_file

# COMMAND ----------


def build_classifier_system_prompt() -> str:
    return f"""You are a compliance analyst for BOE reimbursements on residential operating leases.
All source documents are in Spanish.

Task:
1) Determine whether the document appears to be a BOE-related file, even when it is a basic letter and not a formal invoice.
2) Extract BOE/expense details when possible.
3) Output descriptions in ENGLISH.
4) Classify each expense using: allowable, needs review, not allowable.
5) Base classification on this policy:\n\n{FAM_BOE_GUIDELINES}

Decision guidance:
- A BOE can be a formal invoice, receipt, utility statement, HOA/common charge notice, maintenance summary, landlord letter, or other document that clearly describes a reimbursable lease operating expense.
- If the file is not clearly BOE-related OR there is insufficient evidence to determine the nature of the file, set assessment to "not allowable" and explain exactly why.
- For these cases, assessment_reason MUST begin with: "file does not appear to be a BOE because ..."

Return JSON only in this schema:
{{
  "vendor_name": "",
  "service_description": "",
  "service_type": "",
  "amount": "",
  "currency": "",
  "invoice_id": "",
  "invoice_date": "",
  "allowability_score": 0,
  "assessment": "allowable|needs review|not allowable",
  "assessment_reason": ""
}}
"""


def build_classifier_user_prompt(file_name: str, extracted_text: str, invoice_fields: Dict[str, Any]) -> str:
    return f"""Analyze this Spanish BOE document.

File name: {file_name}
Document text:
{extracted_text[:MAX_TEXT_CHARS]}

Document Intelligence invoice fields (if present):
{json.dumps(invoice_fields, ensure_ascii=False)}

Use the text and fields together. Keep output in English.
If uncertain what the file is, explicitly explain uncertainty in assessment_reason using the required prefix."""


def build_text_triage_system_prompt() -> str:
    return """You are triaging a Spanish document for BOE processing.
Read the extracted text and decide whether additional Document Intelligence invoice field extraction
is likely to add value.

Return JSON only:
{
  "doc_type": "invoice|receipt|utility|letter|unknown",
  "is_boe_related": true,
  "run_document_intelligence": false,
  "triage_reason": ""
}

Set run_document_intelligence=true only when the text suggests invoice-style structured fields
or missing/unclear key data (vendor, amount, date, invoice id) that DI might recover."""


def build_text_triage_user_prompt(file_name: str, extracted_text: str) -> str:
    return f"""File name: {file_name}

Extracted text:
{extracted_text[:MAX_TEXT_CHARS]}

Determine if this is BOE-related and whether to run Document Intelligence."""


def extract_invoice_fields(di_result: Dict[str, Any]) -> Dict[str, Any]:
    if not di_result or "documents" not in di_result or not di_result["documents"]:
        return {}

    fields = di_result["documents"][0].get("fields", {})
    normalized: Dict[str, Any] = {}

    for key in ["VendorName", "InvoiceId", "InvoiceDate", "InvoiceTotal"]:
        value = fields.get(key)
        if not value:
            continue
        if key == "InvoiceTotal" and value.get("valueCurrency"):
            normalized[key] = {
                "amount": value["valueCurrency"].get("amount"),
                "currency": value["valueCurrency"].get("currencyCode"),
            }
        else:
            normalized[key] = value.get("content") or value.get("valueString") or value.get("valueDate")

    return normalized


def run_document_intelligence(file_bytes: bytes) -> Dict[str, Any]:
    """Run multiple Document Intelligence models for broad coverage.

    Supports machine-readable PDFs, scanned files, and handwritten notes.
    """
    model_attempts = ["prebuilt-invoice", "prebuilt-read", "prebuilt-layout"]
    combined: Dict[str, Any] = {"content": "", "documents": []}

    for model_id in model_attempts:
        try:
            result = doc_client.analyze_document(document_bytes=file_bytes, model_id=model_id)
            if result.get("content") and len(result.get("content", "")) > len(combined.get("content", "")):
                combined["content"] = result["content"]

            if result.get("documents"):
                combined["documents"].extend(result["documents"])
        except Exception as model_error:
            print(f"  Document Intelligence model failed ({model_id}): {model_error}")

    return combined


def extract_best_available_text(file_path: str, file_bytes: bytes) -> str:
    """Extract text robustly before any Document Intelligence model is executed."""
    # AI-aware text extraction can route to OCR and handwriting-capable paths.
    ai_text = extract_text(file_bytes, ai=True)
    if ai_text and ai_text.strip():
        return ai_text

    # Vision OCR fallback for image-like docs and hard OCR edge cases.
    vision_text = fh_vision.analyze_text(file_bytes, return_plain_text=True)
    if vision_text and vision_text.strip():
        return vision_text

    # Final fallback: light text extraction from PDF bytes (machine readable content).
    text_fallback = extract_text(file_bytes, ai=False)
    if text_fallback and text_fallback.strip():
        return text_fallback

    # If still empty, provide a marker so downstream LLM has context.
    return f"[No text extracted from {file_path}. Potentially image-only or unsupported format.]"


def translate_if_needed(text: str) -> str:
    if not text:
        return ""
    return translator.translate(text=text, source_language="es", target_language="en") or text


def build_output_row(file_path: str, model_json: Dict[str, Any]) -> Dict[str, Any]:
    desc = model_json.get("service_description", "")
    reason = model_json.get("assessment_reason", "")

    return {
        "file_path": file_path,
        "vendor_name": model_json.get("vendor_name", ""),
        "description_of_service": translate_if_needed(desc),
        "type_of_service": model_json.get("service_type", ""),
        "amount": model_json.get("amount", ""),
        "currency": model_json.get("currency", ""),
        "invoice_id": model_json.get("invoice_id", ""),
        "date": model_json.get("invoice_date", ""),
        "allowability_assessment": model_json.get("assessment", "needs review"),
        "allowability_score": model_json.get("allowability_score", 0),
        "assessment_reason": translate_if_needed(reason),
        "processed_utc": datetime.utcnow().isoformat(),
    }


# COMMAND ----------

# Native SharePoint listing; no custom wrapper around list/download behavior
sp_items = sp_file.ls(SHAREPOINT_FOLDER_PATH, recursive=True, type="file", include_properties=False)
file_paths = [
    item
    if isinstance(item, str)
    else item.get("path")
    or item.get("serverRelativeUrl")
    or item.get("url")
    or item.get("file_path")
    or item.get("name")
    for item in sp_items
]
file_paths = [p for p in file_paths if p and not p.lower().endswith(SKIP_EXTENSIONS)]

print(f"Found {len(file_paths)} non-CSV files in SharePoint folder: {SHAREPOINT_FOLDER_PATH}")

# COMMAND ----------

results: List[Dict[str, Any]] = []
system_prompt = build_classifier_system_prompt()
triage_system_prompt = build_text_triage_system_prompt()

for idx, file_path in enumerate(file_paths, 1):
    print(f"Processing {idx}/{len(file_paths)}: {file_path}")
    try:
        if not file_path.lower().endswith(ALLOWED_EXTENSIONS):
            results.append(
                {
                    "file_path": file_path,
                    "vendor_name": "",
                    "description_of_service": "",
                    "type_of_service": "",
                    "amount": "",
                    "currency": "",
                    "invoice_id": "",
                    "date": "",
                    "allowability_assessment": "needs review",
                    "allowability_score": 0,
                    "assessment_reason": "Skipped file type not currently supported for extraction.",
                    "processed_utc": datetime.utcnow().isoformat(),
                }
            )
            continue

        content = sp_file.download_file(file_path, return_bytes=True)

        # Step 1: Always extract text first.
        extracted_text = extract_best_available_text(file_path, content)

        # Step 2: Have the model decide whether Document Intelligence is needed.
        triage_prompt = build_text_triage_user_prompt(file_path.split("/")[-1], extracted_text)
        triage_response = prompter.chat(
            messages=[
                {"role": "system", "content": triage_system_prompt},
                {"role": "user", "content": triage_prompt},
            ],
            temperature=0.0,
            response_format="json",
            max_tokens=250,
        )
        triage_json = json.loads(triage_response)

        invoice_fields: Dict[str, Any] = {}
        if bool(triage_json.get("run_document_intelligence", False)):
            di_result = run_document_intelligence(content)
            invoice_fields = extract_invoice_fields(di_result)

        user_prompt = build_classifier_user_prompt(file_path.split("/")[-1], extracted_text, invoice_fields)
        response = prompter.chat(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.0,
            response_format="json",
            max_tokens=800,
        )
        model_json = json.loads(response)

        results.append(build_output_row(file_path, model_json))

    except Exception as ex:
        print(f"  ERROR: {ex}")
        results.append(
            {
                "file_path": file_path,
                "vendor_name": "",
                "description_of_service": "",
                "type_of_service": "",
                "amount": "",
                "currency": "",
                "invoice_id": "",
                "date": "",
                "allowability_assessment": "needs review",
                "allowability_score": 0,
                "assessment_reason": f"Processing error: {ex}",
                "processed_utc": datetime.utcnow().isoformat(),
            }
        )

# COMMAND ----------

results_df = pd.DataFrame(results)

display(results_df)

# Save output to SharePoint only (no local file writes)
output_sharepoint_path = f"{SHAREPOINT_FOLDER_PATH.rstrip('/')}/boe_invoice_assessments.csv"
sp_file.upload_bytes(results_df.to_csv(index=False).encode("utf-8"), output_sharepoint_path, overwrite=True)
print(f"Uploaded output to SharePoint: {output_sharepoint_path}")
